---
title: "LWB Strategic Plans"
author: "Amy Shuff"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```


``` {r, results='hide'}

# Only do this once:
# install.packages(c('knitr', 'usethis', 'tidyverse', 'janitor', 'reshape2', 'stringr', 'here', 'readxl', 'pdftools', 'tidytext'))
# install.packages('textdata')

library(knitr)
library(usethis)
library(tidyverse)
library(janitor)
library(reshape2)
library(stringr)
library(here)
library(readxl)
library(pdftools)
library(tidytext)
library(textdata)

```

# Methodology

Each one of the 28 Texas Local Workforce Boards' (LWBs) Strategic Plans were downloaded.

https://www.twc.texas.gov/agency/workforce-development-boards

Questions I have: 

- Should it be called a Workforce Innovation & Opportunity Act (WIOA) plan instead?

```{r PDF Import, include=FALSE}

# PDF Strategic Plans
# Import all pdf files
pdf.file.list <- list.files(pattern='*.pdf', recursive = TRUE)
pdf.list <- sapply(pdf.file.list, pdf_text, simplify = FALSE, USE.NAMES = TRUE)

# pdf.file.list
# pdf.list


# File names to be joined with data later
pdf.names <- data.frame(pdf.file.list) %>% 
  mutate(Board = row_number())
  
pdf.names <- pdf.names %>% 
  cbind(., Region = c("Panhandle", "South Plains", "North Texas", "North Central Texas", "Tarrant County", "Greater Dallas", "Northeast Texas", "East Texas", "West Central Texas", "Borderplex", "Permian-Basin", "Concho Valley", "Heart of Texas", "Capital Area", "Rural Capital Area", "Brazos Valley", "Deep East Texas", "Southeast Texas", "Golden Crescent", "Alamo", "South Texas", "Coastal Bend", "Lower Rio Grande Valley", "Cameron", "Texoma", "Central Texas", "Middle Rio Grande", "Gulf Coast")) 

pdf.names <- pdf.names %>% 
  mutate (
    Year = ifelse(endsWith(pdf.names$pdf.file.list, "2021.pdf"), "2021", ""),
    Year = ifelse(endsWith(pdf.names$pdf.file.list, "2023.pdf"), "2023", Year)
  )

```

- Currently, I need to verify that I have the 2-year modification plans for East Texas, Southeast Texas, South Texas, and Texoma. From looking at their files, I could not see that what I have is updated and not the 2021 version.


```{r Unnest words tidytext, include=FALSE}

# Add all text to list
pdf.text.list <- list()

for(pdf in 1:max(row_number(pdf.file.list))){
    pdf_loop <- tibble(Text = pdf.list[[pdf]]) %>%
      mutate(pdf.file.list = paste(pdf))
    pdf.text.list[[pdf]] <- pdf_loop
}

# Change list to data frame
pdf.text <- bind_rows(list(pdf.text.list), .id = "Board") %>% 
  select(Text, Board) %>% 
  mutate(Board = as.numeric(Board)) %>%
  full_join(pdf.names, by = "Board") %>% 
  group_by(Region) %>%
  mutate(page = row_number()) %>% 
  ungroup() 

# Unnest text
tidy.text <- pdf.text %>% 
      unnest_tokens(word, Text)


```


The TidyText R package is being utilized for this analysis: https://www.tidytextmining.com/tidytext




# Phrase Counts

After importing all the PDFs' text into R, I did a simple count of common phrases by Region.

```{r}

# pdf.text does not have stop words removed 

# Counts how many times this phrase was used by the text
phrases <- pdf.text %>% 
  mutate(OY = str_count(pdf.text$Text, "opportunity youth"),
         DY = str_count(pdf.text$Text, "disconnected youth"),
         Foster = str_count(pdf.text$Text, "foster"),
         HY = str_count(pdf.text$Text, "homeless youth") 
              + str_count(pdf.text$Text, "unhoused youth"),
         OSY = str_count(pdf.text$Text, "out of school")) %>% 
  group_by(Region) %>% 
  summarize(OY = sum(OY),
            DY = sum(DY),
            Foster = sum(Foster),
            Homeless = sum(HY),
            OSY = sum(OSY)) %>% 
  mutate("Total Mentions" = OY + DY)



kable(phrases %>% filter(OY >0) %>% select(Region, OY),
  caption = "Opportunity Youth Mentions")

kable(phrases %>% filter(DY >0) %>% select(Region, DY),
  caption = "Disconnected Youth Mentions")

kable(phrases %>% filter(phrases$`Total Mentions`> 0),
  caption = "Any Related Opportunity Youth Mentions")

```
Four regions used the phrase "opportunity youth" and five regions used the phrase "disconnected youth". Greater Dallas stood out as the region that used these phrases most often.

# Word Count

Stop words (common words such as "it", "the", "to", etc.) were removed. 

- I'm wondering if I should also remove numbers? 0 is one of the most frequently used words.

```{r, Stop Words, include=FALSE}

# Stop words are words that are not useful for an analysis, typically extremely common words such as “the”, “of”, “to”, and so forth in English. We can remove stop words here
data(stop_words) 

# But I don't want to remove young or work (lexicon = onix)
# Should I keep SMART, snowball, or both?
stop_words <- stop_words %>% 
  filter(lexicon == "SMART")

words <- tidy.text %>%
  anti_join(stop_words)

```


I then performed a word count.

```{r}

word.count <- words %>% 
  count(word, sort = TRUE)

kable(word.count %>% head(., 25),
  caption = "25 Most Frequently Used Words")

# UPDATE: add in rank #s?

word.count %>% 
  head(., 25) %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) +
  labs(title = "Most Frequently Used Words",
       subtitle = "for all Texas LWBs combined")
  

```

Youth is the 22nd most used word overall. Fun fact: "Youngsters" was used twice by North Texas.

```{r}

word.count.region <- words %>% 
  group_by(Region) %>% 
  count(word, sort = TRUE) %>% 
  pivot_wider(names_from = Region, values_from = n) %>% 
  full_join(., word.count) %>% 
  arrange(-n)

word.count.region.25 <- word.count.region %>%  
  head(., 25)

kable(word.count.region.25, caption = "Most Frequently Used Words by Region")


word.count.region.25.long <- word.count.region.25 %>% 
  pivot_longer(cols = Cameron:`Brazos Valley`, names_to = "Region", values_to = "count")

# I don't think this is helpful, but it sure is pretty
word.count.region.25.long %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = count, y = word, fill = Region)) +
  geom_col()


```



```{r frequency}

frequency <- words %>% 
  count(Region, word) %>% 
  group_by(Region) %>% 
  mutate(proportion = n/sum(n)) %>% 
  select(-n) %>% 
  arrange(-proportion) 

# | word == "opportunity", fill = word
 

frequency %>% 
  filter(word == "youth") %>% 
  ungroup() %>% 
  mutate(Region = reorder(Region, proportion)) %>%
  mutate(rank = row_number()) %>% 
  ggplot(aes(x = proportion, y = Region)) +
  geom_col() +
  labs(title = "Youth Mentions by Region",
       subtitle = "as a proportion of all words used")


```


# Sentiment Analysis

 Name: AFINN-111 
 URL: http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010 
 License: Open Database License (ODbL) v1.0
 

 Citation info:

This dataset was published in Saif M. Mohammad and Peter Turney. (2013), ``Crowdsourcing a Word-Emotion Association Lexicon.'' Computational Intelligence, 29(3): 436-465.

article{mohammad13,
author = {Mohammad, Saif M. and Turney, Peter D.},
title = {Crowdsourcing a Word-Emotion Association Lexicon},
journal = {Computational Intelligence},
volume = {29},
number = {3},
pages = {436-465},
doi = {10.1111/j.1467-8640.2012.00460.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8640.2012.00460.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8640.2012.00460.x},
year = {2013}
}
If you use this lexicon, then please cite it. 

```{r}

# get_sentiments("afinn")
# get_sentiments("bing")
# get_sentiments("nrc")

# Value (-5 to 5)
sentiment.afinn <- get_sentiments("afinn")

# Positive or Negative
sentiment.bing <- get_sentiments("bing")

# Trust, Fear, Sadness, Anger, Joy, Disgust, Negative, Positive
sentiment.nrc <- get_sentiments("nrc")

# nrc_joy <- get_sentiments("nrc") %>% 
#   filter(sentiment == "joy")
# 
# nrc_joy_region <- words %>%
#   group_by(Region) %>%
#   inner_join(nrc_joy) %>%
#   count(word, sort = TRUE)
# 
# nrc_joy_words <- words %>%
#   inner_join(nrc_joy) %>%
#   count(word, sort = TRUE)


sentiment.count <- words %>%
  inner_join(get_sentiments("bing")) %>%
  group_by(sentiment) %>% 
  count(word, sort = TRUE) 

sentiment.count.25 <- sentiment.count %>% 
  head(., 25)

kable(sentiment.count %>% head(., 25),
  caption = "25 Most Frequently Used Sentiment Words"
)

sentiment.region <- words %>%
  inner_join(get_sentiments("bing")) %>%
  count(Region, index = page %/% 5, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)

```

```{r fig.height=12}

ggplot(sentiment.region, aes(index, sentiment, fill = Region)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Region, ncol = 4, scales = "free_x") +
  labs(title = "Sentiment of Strategic Plans by Region",
       subtitle = "by every 5 pages of narrative")

```

