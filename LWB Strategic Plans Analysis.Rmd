---
title: "LWB Strategic Plans"
author: "Amy Shuff"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```


``` {r, results='hide'}

# Only do this once:
# install.packages(c('knitr', 'usethis', 'tidyverse', 'janitor', 'reshape2', 'stringr', 'here', 'readxl', 'pdftools', 'tidytext'))
# install.packages('textdata')

library(knitr)
library(usethis)
library(tidyverse)
library(janitor)
library(reshape2)
library(stringr)
library(here)
library(readxl)
library(pdftools)
library(tidytext)
library(textdata)

```

# Methodology

Each one of the 28 Texas Local Workforce Boards' (LWBs) Strategic Plans were downloaded.

https://www.twc.texas.gov/agency/workforce-development-boards

Questions I have: 

- Should it be called a Workforce Innovation & Opportunity Act (WIOA) plan instead?

```{r PDF Import, include=FALSE}

# PDF Strategic Plans
# Import all pdf files
pdf.file.list <- list.files(pattern='*.pdf', recursive = TRUE)
pdf.list <- sapply(pdf.file.list, pdf_text, simplify = FALSE, USE.NAMES = TRUE)

# pdf.file.list
# pdf.list


# File names to be joined with data later
pdf.names <- data.frame(pdf.file.list) %>% 
  mutate(Board = row_number())
  
pdf.names <- pdf.names %>% 
  cbind(., Region = c("Panhandle", "South Plains", "North Texas", "North Central Texas", "Tarrant County", "Greater Dallas", "Northeast Texas", "East Texas", "West Central Texas", "Borderplex", "Permian-Basin", "Concho Valley", "Heart of Texas", "Capital Area", "Rural Capital Area", "Brazos Valley", "Deep East Texas", "Southeast Texas", "Golden Crescent", "Alamo", "South Texas", "Coastal Bend", "Lower Rio Grande Valley", "Cameron", "Texoma", "Central Texas", "Middle Rio Grande", "Gulf Coast")) 

pdf.names <- pdf.names %>% 
  mutate (
    Year = ifelse(endsWith(pdf.names$pdf.file.list, "2021.pdf"), "2021", ""),
    Year = ifelse(endsWith(pdf.names$pdf.file.list, "2023.pdf"), "2023", Year)
  )

```

- Currently, I need to verify that I have the 2-year modification plans for East Texas, Southeast Texas, South Texas, and Texoma. From looking at their files, I could not see that what I have is updated and not the 2021 version.


```{r Unnest words tidytext, include=FALSE}

# Add all text to list
pdf.text.list <- list()

for(pdf in 1:max(row_number(pdf.file.list))){
    pdf_loop <- tibble(Text = pdf.list[[pdf]]) %>%
      mutate(pdf.file.list = paste(pdf))
    pdf.text.list[[pdf]] <- pdf_loop
}

# Change list to data frame
pdf.text <- bind_rows(list(pdf.text.list), .id = "Board") %>% 
  select(Text, Board) %>% 
  mutate(Board = as.numeric(Board)) %>%
  full_join(pdf.names, by = "Board") %>% 
  group_by(Region) %>%
  mutate(page = row_number()) %>% 
  ungroup() 

# Unnest text
tidy.text <- pdf.text %>% 
      unnest_tokens(word, Text)


```


The TidyText R package is being utilized for this analysis: https://www.tidytextmining.com/tidytext




# Phrase Counts

After importing all the PDFs' text into R, I did a simple count of common phrases by Region.

```{r Phrase Counts}

# pdf.text does not have stop words removed 

# Counts how many times this phrase was used by the text
phrases <- pdf.text %>% 
  mutate(OY = str_count(pdf.text$Text, "opportunity youth"),
         DY = str_count(pdf.text$Text, "disconnected youth"),
         OSY = str_count(pdf.text$Text, "out of school youth")) %>% 
  group_by(Region) %>% 
  summarize(OY = sum(OY),
            DY = sum(DY),
            OSY = sum(OSY)) %>% 
  mutate("Total Mentions" = OY + DY + OSY) %>% 
  arrange(desc(`Total Mentions`))

kable(phrases %>% filter(phrases$`Total Mentions` > 0),
  caption = "Any Related Opportunity Youth Mentions")

```

"Unhoused" is not used at all in any text.

Alamo mentions homeless youth but it's not being captured by the code. It has been imported as "homeless                           youth"

```{r Risk Factors}

Homeless <- pdf.text %>% 
  mutate(Homeless = str_count(pdf.text$Text, "homeless"),
         HY = str_count(pdf.text$Text, "homeless youth"),
         Unhoused = str_count(pdf.text$Text, "unhoused")) %>% 
  group_by(Region) %>% 
  summarize(Homeless = sum(Homeless),
            HY = sum(HY),
            Unhoused = sum(Unhoused)) %>% 
  mutate("Homeless Mentions" = Homeless + HY + Unhoused) %>% 
  arrange(desc(`Homeless Mentions`))

kable(Homeless %>% filter(`Homeless Mentions` > 0),
  caption = "Homeless Mentions")


# homeless.text <- pdf.text %>% 
#   mutate(Homeless = str_detect(pdf.text$Text, "homeless"))
# 
# kable(homeless.text %>% filter(Homeless == TRUE) %>% select(Region, Text),
#   caption = "Text") 


# Table for OY synonyms: OY, DY, OSY
# Tables for related risk factors: foster, homeless, at-risk, youth offenders, juvenile justice
# Table for mentioned age ranges

```




```{r}

kable(phrases %>% filter(OY >0) %>% select(Region, OY),
  caption = "Opportunity Youth Mentions")

kable(phrases %>% filter(DY >0) %>% select(Region, DY),
  caption = "Disconnected Youth Mentions")

```
Four regions used the phrase "opportunity youth" and five regions used the phrase "disconnected youth". Greater Dallas stood out as the region that used these phrases most often.

# Word Count

Stop words (common words such as "it", "the", "to", etc.) were removed. 

- I'm wondering if I should also remove numbers? 0 is one of the most frequently used words.

```{r, Stop Words, include=FALSE}

# Stop words are words that are not useful for an analysis, typically extremely common words such as “the”, “of”, “to”, and so forth in English. We can remove stop words here
data(stop_words) 

# But I don't want to remove young or work (lexicon = onix)
# Should I keep SMART, snowball, or both?
stop_words <- stop_words %>% 
  filter(lexicon == "SMART")

words <- tidy.text %>%
  anti_join(stop_words)

```


I then performed a word count.

```{r}

word.count.total <- words %>% 
  count(word, sort = TRUE) %>% 
  mutate(rank = row_number())

kable(word.count.total %>% head(., 25),
  caption = "25 Most Frequently Used Words")


word.count.total %>% 
  head(., 25) %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) +
  labs(title = "Most Frequently Used Words",
       subtitle = "for all Texas LWBs combined")
  

```

Youth is the 22nd most used word overall. Fun fact: "Youngsters" was used twice by North Texas.

```{r}

word.count.region <- words %>% 
  group_by(Region) %>% 
  count(word, sort = TRUE) %>% 
  pivot_wider(names_from = Region, values_from = n) %>% 
  full_join(., word.count.total) %>% 
  arrange(-n)

word.count.region.25 <- word.count.region %>%  
  head(., 25)

kable(word.count.region.25, caption = "Most Frequently Used Words by Region")


word.count.region.25.long <- word.count.region.25 %>% 
  pivot_longer(cols = Cameron:`Brazos Valley`, names_to = "Region", values_to = "count")

# I don't think this is helpful, but it sure is pretty
word.count.region.25.long %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = count, y = word, fill = Region)) +
  geom_col()


```



```{r frequency}

frequency <- words %>% 
  count(Region, word) %>% 
  group_by(Region) %>% 
  mutate(proportion = n/sum(n)) %>% 
  select(-n) %>% 
  arrange(-proportion) 

# | word == "opportunity", fill = word
 

frequency %>% 
  filter(word == "youth") %>% 
  ungroup() %>% 
  mutate(Region = reorder(Region, proportion)) %>%
  mutate(rank = row_number()) %>% 
  ggplot(aes(x = proportion, y = Region)) +
  geom_col() +
  labs(title = "Youth Mentions by Region",
       subtitle = "as a proportion of all words used")


```


# Sentiment Analysis

 Name: AFINN-111 
 URL: http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010 
 License: Open Database License (ODbL) v1.0
 

 Citation info:

This dataset was published in Saif M. Mohammad and Peter Turney. (2013), ``Crowdsourcing a Word-Emotion Association Lexicon.'' Computational Intelligence, 29(3): 436-465.

article{mohammad13,
author = {Mohammad, Saif M. and Turney, Peter D.},
title = {Crowdsourcing a Word-Emotion Association Lexicon},
journal = {Computational Intelligence},
volume = {29},
number = {3},
pages = {436-465},
doi = {10.1111/j.1467-8640.2012.00460.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8640.2012.00460.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8640.2012.00460.x},
year = {2013}
}
If you use this lexicon, then please cite it. 

```{r}

# get_sentiments("afinn")
# get_sentiments("bing")
# get_sentiments("nrc")

# Value (-5 to 5)
sentiment.afinn <- get_sentiments("afinn")

# Positive or Negative
sentiment.bing <- get_sentiments("bing")

# Trust, Fear, Sadness, Anger, Joy, Disgust, Negative, Positive
sentiment.nrc <- get_sentiments("nrc")

# nrc_joy <- get_sentiments("nrc") %>% 
#   filter(sentiment == "joy")
# 
# nrc_joy_region <- words %>%
#   group_by(Region) %>%
#   inner_join(nrc_joy) %>%
#   count(word, sort = TRUE)
# 
# nrc_joy_words <- words %>%
#   inner_join(nrc_joy) %>%
#   count(word, sort = TRUE)


sentiment.count <- words %>%
  inner_join(get_sentiments("bing")) %>%
  group_by(sentiment) %>% 
  count(word, sort = TRUE) 

sentiment.count.25 <- sentiment.count %>% 
  head(., 25)

kable(sentiment.count %>% head(., 25),
  caption = "25 Most Frequently Used Sentiment Words"
)

sentiment.region <- words %>%
  inner_join(get_sentiments("bing")) %>%
  count(Region, index = page %/% 5, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)

```

```{r fig.height=12}

ggplot(sentiment.region, aes(index, sentiment, fill = Region)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Region, ncol = 4, scales = "free_x") +
  labs(title = "Sentiment of Strategic Plans by Region",
       subtitle = "by every 5 pages of narrative")

```

# Term Frequency - Inverse Document Frequency (tf-idf)

"The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents."

None of these words occur in all of the texts. They are important, characteristic words for each region.

```{r}

word.count <- words %>% 
  mutate(word = gsub(x = word, pattern = "[0-9]", replacement = "")) %>% 
  count(Region, word, sort = TRUE)

# I want to remove numbers

#mutate(mydata, text = gsub(x = text, pattern = "[0-9]+|[[:punct:]]|\\(.*\\)", replacement = "")) 

#gsub(x = mydata$text, pattern = "[0-9]+|[[:punct:]]|\\(.*\\)", replacement = "")


# total.words <- word.count %>% 
#   group_by(Region) %>% 
#   summarize(total = sum(n))  
# 
# word.count <- left_join(word.count, total.words)

word.tf_idf <- word.count %>% 
  bind_tf_idf(word, Region, n) %>%
  arrange(desc(tf_idf))

```

```{r fig.height=24, fig.width=12}

word.tf_idf %>%
  group_by(Region) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = Region)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Region, ncol = 4, scales = "free") +
  labs(x = "tf-idf", y = NULL)

```

